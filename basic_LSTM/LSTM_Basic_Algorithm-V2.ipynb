{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Load LSTM network and generate text\n",
    "import sys\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ascii text and covert to lowercase\n",
    "filename = \"alice.txt\"\n",
    "raw_text = open(filename).read()\n",
    "raw_text = raw_text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mapping of unique chars to integers, and a reverse mapping\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  2893\n",
      "Total Vocab:  47\n"
     ]
    }
   ],
   "source": [
    "# summarize the loaded data\n",
    "n_chars = int(len(raw_text)/50)\n",
    "n_vocab = len(chars)\n",
    "print( \"Total Characters: \", n_chars)\n",
    "print( \"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  2793\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print( \"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X.SHAPE IS THE LENGTH OF THE DESIRE RETURN IF I WANT TO RETURN \n",
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "# Adding a return_sequence allow the next layer to have a 3D sequence input\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the network weights\n",
    "# filename = \"weights-improvement-19-1.9435.hdf5\"\n",
    "# model.load_weights(filename)\n",
    "\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}-bigger2.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/70\n",
      "2793/2793 [==============================] - 31s 11ms/step - loss: 3.1679\n",
      "\n",
      "Epoch 00001: loss improved from inf to 3.16793, saving model to weights-improvement-01-3.1679-bigger2.hdf5\n",
      "Epoch 2/70\n",
      "2793/2793 [==============================] - 29s 10ms/step - loss: 3.0378\n",
      "\n",
      "Epoch 00002: loss improved from 3.16793 to 3.03778, saving model to weights-improvement-02-3.0378-bigger2.hdf5\n",
      "Epoch 3/70\n",
      "2793/2793 [==============================] - 29s 10ms/step - loss: 3.0348\n",
      "\n",
      "Epoch 00003: loss improved from 3.03778 to 3.03484, saving model to weights-improvement-03-3.0348-bigger2.hdf5\n",
      "Epoch 4/70\n",
      "2793/2793 [==============================] - 29s 10ms/step - loss: 3.0268\n",
      "\n",
      "Epoch 00004: loss improved from 3.03484 to 3.02677, saving model to weights-improvement-04-3.0268-bigger2.hdf5\n",
      "Epoch 5/70\n",
      "2793/2793 [==============================] - 29s 10ms/step - loss: 3.0189\n",
      "\n",
      "Epoch 00005: loss improved from 3.02677 to 3.01893, saving model to weights-improvement-05-3.0189-bigger2.hdf5\n",
      "Epoch 6/70\n",
      "2793/2793 [==============================] - 29s 10ms/step - loss: 3.0104\n",
      "\n",
      "Epoch 00006: loss improved from 3.01893 to 3.01042, saving model to weights-improvement-06-3.0104-bigger2.hdf5\n",
      "Epoch 7/70\n",
      "2793/2793 [==============================] - 29s 10ms/step - loss: 3.0122\n",
      "\n",
      "Epoch 00007: loss did not improve from 3.01042\n",
      "Epoch 8/70\n",
      "2793/2793 [==============================] - 29s 10ms/step - loss: 2.9958\n",
      "\n",
      "Epoch 00008: loss improved from 3.01042 to 2.99576, saving model to weights-improvement-08-2.9958-bigger2.hdf5\n",
      "Epoch 9/70\n",
      "2793/2793 [==============================] - 29s 10ms/step - loss: 2.9717\n",
      "\n",
      "Epoch 00009: loss improved from 2.99576 to 2.97169, saving model to weights-improvement-09-2.9717-bigger2.hdf5\n",
      "Epoch 10/70\n",
      "2793/2793 [==============================] - 29s 10ms/step - loss: 2.9516\n",
      "\n",
      "Epoch 00010: loss improved from 2.97169 to 2.95158, saving model to weights-improvement-10-2.9516-bigger2.hdf5\n",
      "Epoch 11/70\n",
      "2793/2793 [==============================] - 29s 10ms/step - loss: 2.9163\n",
      "\n",
      "Epoch 00011: loss improved from 2.95158 to 2.91628, saving model to weights-improvement-11-2.9163-bigger2.hdf5\n",
      "Epoch 12/70\n",
      "2793/2793 [==============================] - 29s 10ms/step - loss: 2.8903\n",
      "\n",
      "Epoch 00012: loss improved from 2.91628 to 2.89028, saving model to weights-improvement-12-2.8903-bigger2.hdf5\n",
      "Epoch 13/70\n",
      "2793/2793 [==============================] - 29s 10ms/step - loss: 2.8572\n",
      "\n",
      "Epoch 00013: loss improved from 2.89028 to 2.85718, saving model to weights-improvement-13-2.8572-bigger2.hdf5\n",
      "Epoch 14/70\n",
      "2793/2793 [==============================] - 29s 10ms/step - loss: 2.8299\n",
      "\n",
      "Epoch 00014: loss improved from 2.85718 to 2.82991, saving model to weights-improvement-14-2.8299-bigger2.hdf5\n",
      "Epoch 15/70\n",
      "2793/2793 [==============================] - 29s 10ms/step - loss: 2.8103\n",
      "\n",
      "Epoch 00015: loss improved from 2.82991 to 2.81034, saving model to weights-improvement-15-2.8103-bigger2.hdf5\n",
      "Epoch 16/70\n",
      "2793/2793 [==============================] - 29s 10ms/step - loss: 2.7855\n",
      "\n",
      "Epoch 00016: loss improved from 2.81034 to 2.78554, saving model to weights-improvement-16-2.7855-bigger2.hdf5\n",
      "Epoch 17/70\n",
      "2793/2793 [==============================] - 29s 10ms/step - loss: 2.7674\n",
      "\n",
      "Epoch 00017: loss improved from 2.78554 to 2.76739, saving model to weights-improvement-17-2.7674-bigger2.hdf5\n",
      "Epoch 18/70\n",
      "2793/2793 [==============================] - 29s 10ms/step - loss: 2.7398\n",
      "\n",
      "Epoch 00018: loss improved from 2.76739 to 2.73978, saving model to weights-improvement-18-2.7398-bigger2.hdf5\n",
      "Epoch 19/70\n",
      "2793/2793 [==============================] - 29s 10ms/step - loss: 2.7141\n",
      "\n",
      "Epoch 00019: loss improved from 2.73978 to 2.71412, saving model to weights-improvement-19-2.7141-bigger2.hdf5\n",
      "Epoch 20/70\n",
      "2793/2793 [==============================] - 29s 10ms/step - loss: 2.6893\n",
      "\n",
      "Epoch 00020: loss improved from 2.71412 to 2.68927, saving model to weights-improvement-20-2.6893-bigger2.hdf5\n",
      "Epoch 21/70\n",
      "2793/2793 [==============================] - 29s 10ms/step - loss: 2.6668\n",
      "\n",
      "Epoch 00021: loss improved from 2.68927 to 2.66679, saving model to weights-improvement-21-2.6668-bigger2.hdf5\n",
      "Epoch 22/70\n",
      "2793/2793 [==============================] - 29s 10ms/step - loss: 2.6603\n",
      "\n",
      "Epoch 00022: loss improved from 2.66679 to 2.66026, saving model to weights-improvement-22-2.6603-bigger2.hdf5\n",
      "Epoch 23/70\n",
      "2793/2793 [==============================] - 29s 10ms/step - loss: 2.6210\n",
      "\n",
      "Epoch 00023: loss improved from 2.66026 to 2.62101, saving model to weights-improvement-23-2.6210-bigger2.hdf5\n",
      "Epoch 24/70\n",
      "2793/2793 [==============================] - 29s 10ms/step - loss: 2.6016\n",
      "\n",
      "Epoch 00024: loss improved from 2.62101 to 2.60159, saving model to weights-improvement-24-2.6016-bigger2.hdf5\n",
      "Epoch 25/70\n",
      "2793/2793 [==============================] - 29s 10ms/step - loss: 2.5729\n",
      "\n",
      "Epoch 00025: loss improved from 2.60159 to 2.57290, saving model to weights-improvement-25-2.5729-bigger2.hdf5\n",
      "Epoch 26/70\n",
      "2793/2793 [==============================] - 29s 10ms/step - loss: 2.5421\n",
      "\n",
      "Epoch 00026: loss improved from 2.57290 to 2.54207, saving model to weights-improvement-26-2.5421-bigger2.hdf5\n",
      "Epoch 27/70\n",
      "2793/2793 [==============================] - 29s 10ms/step - loss: 2.5061\n",
      "\n",
      "Epoch 00027: loss improved from 2.54207 to 2.50609, saving model to weights-improvement-27-2.5061-bigger2.hdf5\n",
      "Epoch 28/70\n",
      "2793/2793 [==============================] - 29s 10ms/step - loss: 2.4760\n",
      "\n",
      "Epoch 00028: loss improved from 2.50609 to 2.47603, saving model to weights-improvement-28-2.4760-bigger2.hdf5\n",
      "Epoch 29/70\n",
      "2793/2793 [==============================] - 29s 10ms/step - loss: 2.4275\n",
      "\n",
      "Epoch 00029: loss improved from 2.47603 to 2.42745, saving model to weights-improvement-29-2.4275-bigger2.hdf5\n",
      "Epoch 30/70\n",
      "2793/2793 [==============================] - 29s 10ms/step - loss: 2.3805\n",
      "\n",
      "Epoch 00030: loss improved from 2.42745 to 2.38053, saving model to weights-improvement-30-2.3805-bigger2.hdf5\n",
      "Epoch 31/70\n",
      "2793/2793 [==============================] - 29s 10ms/step - loss: 2.3153\n",
      "\n",
      "Epoch 00031: loss improved from 2.38053 to 2.31532, saving model to weights-improvement-31-2.3153-bigger2.hdf5\n",
      "Epoch 32/70\n",
      "2793/2793 [==============================] - 29s 10ms/step - loss: 2.2804\n",
      "\n",
      "Epoch 00032: loss improved from 2.31532 to 2.28035, saving model to weights-improvement-32-2.2804-bigger2.hdf5\n",
      "Epoch 33/70\n",
      "2793/2793 [==============================] - 29s 10ms/step - loss: 2.2150\n",
      "\n",
      "Epoch 00033: loss improved from 2.28035 to 2.21495, saving model to weights-improvement-33-2.2150-bigger2.hdf5\n",
      "Epoch 34/70\n",
      "2793/2793 [==============================] - 29s 10ms/step - loss: 2.1382\n",
      "\n",
      "Epoch 00034: loss improved from 2.21495 to 2.13820, saving model to weights-improvement-34-2.1382-bigger2.hdf5\n",
      "Epoch 35/70\n",
      "2793/2793 [==============================] - 29s 10ms/step - loss: 2.0721\n",
      "\n",
      "Epoch 00035: loss improved from 2.13820 to 2.07214, saving model to weights-improvement-35-2.0721-bigger2.hdf5\n",
      "Epoch 36/70\n",
      "2793/2793 [==============================] - 29s 10ms/step - loss: 1.9722\n",
      "\n",
      "Epoch 00036: loss improved from 2.07214 to 1.97217, saving model to weights-improvement-36-1.9722-bigger2.hdf5\n",
      "Epoch 37/70\n",
      "2793/2793 [==============================] - 29s 10ms/step - loss: 1.8978\n",
      "\n",
      "Epoch 00037: loss improved from 1.97217 to 1.89778, saving model to weights-improvement-37-1.8978-bigger2.hdf5\n",
      "Epoch 38/70\n",
      "2793/2793 [==============================] - 29s 10ms/step - loss: 1.8070\n",
      "\n",
      "Epoch 00038: loss improved from 1.89778 to 1.80697, saving model to weights-improvement-38-1.8070-bigger2.hdf5\n",
      "Epoch 39/70\n",
      "2793/2793 [==============================] - 28s 10ms/step - loss: 1.7091\n",
      "\n",
      "Epoch 00039: loss improved from 1.80697 to 1.70915, saving model to weights-improvement-39-1.7091-bigger2.hdf5\n",
      "Epoch 40/70\n",
      "2793/2793 [==============================] - 29s 10ms/step - loss: 1.6042\n",
      "\n",
      "Epoch 00040: loss improved from 1.70915 to 1.60421, saving model to weights-improvement-40-1.6042-bigger2.hdf5\n",
      "Epoch 41/70\n",
      "2793/2793 [==============================] - 29s 10ms/step - loss: 1.5055\n",
      "\n",
      "Epoch 00041: loss improved from 1.60421 to 1.50554, saving model to weights-improvement-41-1.5055-bigger2.hdf5\n",
      "Epoch 42/70\n",
      "2793/2793 [==============================] - 29s 10ms/step - loss: 1.3860\n",
      "\n",
      "Epoch 00042: loss improved from 1.50554 to 1.38601, saving model to weights-improvement-42-1.3860-bigger2.hdf5\n",
      "Epoch 43/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2793/2793 [==============================] - 29s 10ms/step - loss: 1.2825\n",
      "\n",
      "Epoch 00043: loss improved from 1.38601 to 1.28254, saving model to weights-improvement-43-1.2825-bigger2.hdf5\n",
      "Epoch 44/70\n",
      "2793/2793 [==============================] - 29s 10ms/step - loss: 1.2013\n",
      "\n",
      "Epoch 00044: loss improved from 1.28254 to 1.20132, saving model to weights-improvement-44-1.2013-bigger2.hdf5\n",
      "Epoch 45/70\n",
      "2793/2793 [==============================] - 29s 10ms/step - loss: 1.1047\n",
      "\n",
      "Epoch 00045: loss improved from 1.20132 to 1.10473, saving model to weights-improvement-45-1.1047-bigger2.hdf5\n",
      "Epoch 46/70\n",
      "2793/2793 [==============================] - 29s 10ms/step - loss: 1.0183\n",
      "\n",
      "Epoch 00046: loss improved from 1.10473 to 1.01828, saving model to weights-improvement-46-1.0183-bigger2.hdf5\n",
      "Epoch 47/70\n",
      "2793/2793 [==============================] - 29s 10ms/step - loss: 0.9091\n",
      "\n",
      "Epoch 00047: loss improved from 1.01828 to 0.90908, saving model to weights-improvement-47-0.9091-bigger2.hdf5\n",
      "Epoch 48/70\n",
      "2793/2793 [==============================] - 29s 10ms/step - loss: 0.8282\n",
      "\n",
      "Epoch 00048: loss improved from 0.90908 to 0.82819, saving model to weights-improvement-48-0.8282-bigger2.hdf5\n",
      "Epoch 49/70\n",
      "2793/2793 [==============================] - 29s 10ms/step - loss: 0.7438\n",
      "\n",
      "Epoch 00049: loss improved from 0.82819 to 0.74385, saving model to weights-improvement-49-0.7438-bigger2.hdf5\n",
      "Epoch 50/70\n",
      "2793/2793 [==============================] - 29s 10ms/step - loss: 0.6569\n",
      "\n",
      "Epoch 00050: loss improved from 0.74385 to 0.65692, saving model to weights-improvement-50-0.6569-bigger2.hdf5\n",
      "Epoch 51/70\n",
      "2793/2793 [==============================] - 29s 10ms/step - loss: 0.5884\n",
      "\n",
      "Epoch 00051: loss improved from 0.65692 to 0.58838, saving model to weights-improvement-51-0.5884-bigger2.hdf5\n",
      "Epoch 52/70\n",
      "2793/2793 [==============================] - 29s 10ms/step - loss: 0.5463\n",
      "\n",
      "Epoch 00052: loss improved from 0.58838 to 0.54629, saving model to weights-improvement-52-0.5463-bigger2.hdf5\n",
      "Epoch 53/70\n",
      "2793/2793 [==============================] - 29s 10ms/step - loss: 0.4734\n",
      "\n",
      "Epoch 00053: loss improved from 0.54629 to 0.47335, saving model to weights-improvement-53-0.4734-bigger2.hdf5\n",
      "Epoch 54/70\n",
      "2793/2793 [==============================] - 29s 10ms/step - loss: 0.4114\n",
      "\n",
      "Epoch 00054: loss improved from 0.47335 to 0.41139, saving model to weights-improvement-54-0.4114-bigger2.hdf5\n",
      "Epoch 55/70\n",
      "2793/2793 [==============================] - 28s 10ms/step - loss: 0.3842\n",
      "\n",
      "Epoch 00055: loss improved from 0.41139 to 0.38416, saving model to weights-improvement-55-0.3842-bigger2.hdf5\n",
      "Epoch 56/70\n",
      "2793/2793 [==============================] - 29s 10ms/step - loss: 0.3130\n",
      "\n",
      "Epoch 00056: loss improved from 0.38416 to 0.31297, saving model to weights-improvement-56-0.3130-bigger2.hdf5\n",
      "Epoch 57/70\n",
      "2793/2793 [==============================] - 28s 10ms/step - loss: 0.2971\n",
      "\n",
      "Epoch 00057: loss improved from 0.31297 to 0.29708, saving model to weights-improvement-57-0.2971-bigger2.hdf5\n",
      "Epoch 58/70\n",
      "2793/2793 [==============================] - 29s 10ms/step - loss: 0.2567\n",
      "\n",
      "Epoch 00058: loss improved from 0.29708 to 0.25670, saving model to weights-improvement-58-0.2567-bigger2.hdf5\n",
      "Epoch 59/70\n",
      "2793/2793 [==============================] - 28s 10ms/step - loss: 0.2225\n",
      "\n",
      "Epoch 00059: loss improved from 0.25670 to 0.22250, saving model to weights-improvement-59-0.2225-bigger2.hdf5\n",
      "Epoch 60/70\n",
      "2793/2793 [==============================] - 28s 10ms/step - loss: 0.2093\n",
      "\n",
      "Epoch 00060: loss improved from 0.22250 to 0.20928, saving model to weights-improvement-60-0.2093-bigger2.hdf5\n",
      "Epoch 61/70\n",
      "2793/2793 [==============================] - 29s 10ms/step - loss: 0.1876\n",
      "\n",
      "Epoch 00061: loss improved from 0.20928 to 0.18760, saving model to weights-improvement-61-0.1876-bigger2.hdf5\n",
      "Epoch 62/70\n",
      "2793/2793 [==============================] - 29s 10ms/step - loss: 0.1667\n",
      "\n",
      "Epoch 00062: loss improved from 0.18760 to 0.16675, saving model to weights-improvement-62-0.1667-bigger2.hdf5\n",
      "Epoch 63/70\n",
      "2793/2793 [==============================] - 29s 10ms/step - loss: 0.1502\n",
      "\n",
      "Epoch 00063: loss improved from 0.16675 to 0.15019, saving model to weights-improvement-63-0.1502-bigger2.hdf5\n",
      "Epoch 64/70\n",
      "2793/2793 [==============================] - 29s 10ms/step - loss: 0.1337\n",
      "\n",
      "Epoch 00064: loss improved from 0.15019 to 0.13369, saving model to weights-improvement-64-0.1337-bigger2.hdf5\n",
      "Epoch 65/70\n",
      "2793/2793 [==============================] - 29s 10ms/step - loss: 0.1239\n",
      "\n",
      "Epoch 00065: loss improved from 0.13369 to 0.12392, saving model to weights-improvement-65-0.1239-bigger2.hdf5\n",
      "Epoch 66/70\n",
      "2793/2793 [==============================] - 29s 10ms/step - loss: 0.1131\n",
      "\n",
      "Epoch 00066: loss improved from 0.12392 to 0.11309, saving model to weights-improvement-66-0.1131-bigger2.hdf5\n",
      "Epoch 67/70\n",
      "2793/2793 [==============================] - 29s 10ms/step - loss: 0.1028\n",
      "\n",
      "Epoch 00067: loss improved from 0.11309 to 0.10278, saving model to weights-improvement-67-0.1028-bigger2.hdf5\n",
      "Epoch 68/70\n",
      "2793/2793 [==============================] - 29s 10ms/step - loss: 0.1023\n",
      "\n",
      "Epoch 00068: loss improved from 0.10278 to 0.10228, saving model to weights-improvement-68-0.1023-bigger2.hdf5\n",
      "Epoch 69/70\n",
      "2793/2793 [==============================] - 29s 10ms/step - loss: 0.0917\n",
      "\n",
      "Epoch 00069: loss improved from 0.10228 to 0.09169, saving model to weights-improvement-69-0.0917-bigger2.hdf5\n",
      "Epoch 70/70\n",
      "2793/2793 [==============================] - 29s 10ms/step - loss: 0.0836\n",
      "\n",
      "Epoch 00070: loss improved from 0.09169 to 0.08364, saving model to weights-improvement-70-0.0836-bigger2.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0251fb37b8>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model\n",
    "model.fit(X, y, epochs=70, batch_size=64, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model.save('modelo_idiota.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" nothing of tumbling down stairs! how brave they’ll all think me at\n",
      "home! why, i wouldn’t say anythin \"\n"
     ]
    }
   ],
   "source": [
    "# pick a random seed\n",
    "start = np.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print( \"Seed:\")\n",
    "print( \"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[34, 17, 18, 18]\n",
      "4\n",
      "Seed:\n",
      "\" rabb \"\n"
     ]
    }
   ],
   "source": [
    "# pick a random seed\n",
    "start = np.random.randint(0, len(dataX)-1)\n",
    "palavra = 'rabb'\n",
    "pattern = [char_to_int[value] for value in palavra]\n",
    "print(pattern)\n",
    "print(len(pattern))\n",
    "print( \"Seed:\")\n",
    "print( \"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected lstm_5_input to have shape (100, 1) but got array with shape (4, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-103-370d5f15c87c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint_to_char\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1145\u001b[0m                              'argument.')\n\u001b[1;32m   1146\u001b[0m         \u001b[0;31m# Validate user data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    747\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 749\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    135\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    138\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected lstm_5_input to have shape (100, 1) but got array with shape (4, 1)"
     ]
    }
   ],
   "source": [
    "# generate characters\n",
    "# Ler 5 anteriores para prever 1\n",
    "# ir passo a passo vendo a melhoria gerada com o script do fil\n",
    "for i in range(100):\n",
    "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = np .argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print( \"\\nDone.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected lstm_5_input to have shape (100, 1) but got array with shape (4, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-100-d87490eeec2c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0;31m#  index = np .argmax(prediction)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;31m#result = int_to_char[index]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1145\u001b[0m                              'argument.')\n\u001b[1;32m   1146\u001b[0m         \u001b[0;31m# Validate user data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    747\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 749\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    135\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    138\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected lstm_5_input to have shape (100, 1) but got array with shape (4, 1)"
     ]
    }
   ],
   "source": [
    "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "  #  index = np .argmax(prediction)\n",
    "  #result = int_to_char[index]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
